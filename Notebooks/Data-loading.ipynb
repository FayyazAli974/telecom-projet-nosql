{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SQLContext, SparkSession}\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SQLContext, SparkSession}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@322e2aa\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@21f42ccf\n",
       "import spark.implicits._\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val spark = SparkSession\n",
    "      .builder\n",
    "      .config(conf)\n",
    "      .appName(\"TP Spark : Preprocessor\")\n",
    "      .getOrCreate()\n",
    "\n",
    "import spark.implicits._  // to use the symbol $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download masterfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "warning: there was one feature warning; re-run with -feature for details\n",
       "import sys.process._\n",
       "import java.net.URL\n",
       "import java.io.File\n",
       "import java.io.File\n",
       "import java.nio.file.{Files, StandardCopyOption}\n",
       "import java.net.HttpURLConnection\n",
       "import org.apache.spark.sql.functions._\n",
       "fileDownloader: (urlOfFileToDownload: String, fileName: String)Any\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "import java.net.URL\n",
    "import java.io.File\n",
    "import java.io.File\n",
    "import java.nio.file.{Files, StandardCopyOption}\n",
    "import java.net.HttpURLConnection \n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def fileDownloader(urlOfFileToDownload: String, fileName: String) = {\n",
    "    val url = new URL(urlOfFileToDownload)\n",
    "    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\n",
    "    connection.setConnectTimeout(5000)\n",
    "    connection.setReadTimeout(5000)\n",
    "    connection.connect()\n",
    "\n",
    "    if (connection.getResponseCode >= 400)\n",
    "        println(\"error\")\n",
    "    else\n",
    "        url #> new File(fileName) !!\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Any = \"\"\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\", \"./data/masterfilelist.txt\") // save the list file to the Spark Master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data for one day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SQLContext\n",
       "sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@3eab5610\n",
       "filesDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "val sqlContext = new SQLContext(sc)\n",
    "val filesDF = sqlContext.read.\n",
    "                    option(\"delimiter\",\" \").\n",
    "                    option(\"infer_schema\",\"true\").\n",
    "                    csv(\"./data/masterfilelist.txt\").\n",
    "                    withColumnRenamed(\"_c0\",\"size\").\n",
    "                    withColumnRenamed(\"_c1\",\"hash\").\n",
    "                    withColumnRenamed(\"_c2\",\"url\").\n",
    "                    cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------------------+--------------------------------------------------------------------+\n",
      "|size    |hash                            |url                                                                 |\n",
      "+--------+--------------------------------+--------------------------------------------------------------------+\n",
      "|150383  |297a16b493de7cf6ca809a7cc31d0b93|http://data.gdeltproject.org/gdeltv2/20150218230000.export.CSV.zip  |\n",
      "|318084  |bb27f78ba45f69a17ea6ed7755e9f8ff|http://data.gdeltproject.org/gdeltv2/20150218230000.mentions.CSV.zip|\n",
      "|10768507|ea8dde0beb0ba98810a92db068c0ce99|http://data.gdeltproject.org/gdeltv2/20150218230000.gkg.csv.zip     |\n",
      "|149211  |2a91041d7e72b0fc6a629e2ff867b240|http://data.gdeltproject.org/gdeltv2/20150218231500.export.CSV.zip  |\n",
      "|339037  |dec3f427076b716a8112b9086c342523|http://data.gdeltproject.org/gdeltv2/20150218231500.mentions.CSV.zip|\n",
      "|10269336|2f1a504a3c4558694ade0442e9a5ae6f|http://data.gdeltproject.org/gdeltv2/20150218231500.gkg.csv.zip     |\n",
      "|149723  |12268e821823aae2da90882621feda18|http://data.gdeltproject.org/gdeltv2/20150218233000.export.CSV.zip  |\n",
      "|357229  |744acad14559f2781a8db67715d63872|http://data.gdeltproject.org/gdeltv2/20150218233000.mentions.CSV.zip|\n",
      "|11279827|66b03e2efd7d51dabf916b1666910053|http://data.gdeltproject.org/gdeltv2/20150218233000.gkg.csv.zip     |\n",
      "|158842  |a5298ce3c6df1a8a759c61b5c0b6f8bb|http://data.gdeltproject.org/gdeltv2/20150218234500.export.CSV.zip  |\n",
      "|374528  |dd322c888f28311aca2c735468405551|http://data.gdeltproject.org/gdeltv2/20150218234500.mentions.CSV.zip|\n",
      "|11212939|cd20f295649b214dd16666ca451b9994|http://data.gdeltproject.org/gdeltv2/20150218234500.gkg.csv.zip     |\n",
      "|362610  |c4268d558bb22c02b3c132c17818c68b|http://data.gdeltproject.org/gdeltv2/20150219000000.export.CSV.zip  |\n",
      "|287807  |e7f464a7a451ad2af6e9c8fa24f0ccea|http://data.gdeltproject.org/gdeltv2/20150219000000.mentions.CSV.zip|\n",
      "|9728953 |8f4b26e134bd6605cce2d32e92e5d3d7|http://data.gdeltproject.org/gdeltv2/20150219000000.gkg.csv.zip     |\n",
      "|251605  |7685a6c71f010918f3be0d4ed2be977e|http://data.gdeltproject.org/gdeltv2/20150219001500.export.CSV.zip  |\n",
      "|263793  |e23ee65a60a1577dc74b979a54da406e|http://data.gdeltproject.org/gdeltv2/20150219001500.mentions.CSV.zip|\n",
      "|9459370 |6031464dfdcb331551d491916d400c18|http://data.gdeltproject.org/gdeltv2/20150219001500.gkg.csv.zip     |\n",
      "|255259  |f41066efb05d4024fca9dc1c2c6b9112|http://data.gdeltproject.org/gdeltv2/20150219003000.export.CSV.zip  |\n",
      "|308019  |061133d1efd29c66c7ecba0d52063927|http://data.gdeltproject.org/gdeltv2/20150219003000.mentions.CSV.zip|\n",
      "+--------+--------------------------------+--------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filesDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [size: string, hash: string ... 1 more field]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleDF = filesDF.filter(col(\"url\").contains(\"/20181201\")).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+\n",
      "|    size|                hash|                 url|\n",
      "+--------+--------------------+--------------------+\n",
      "|  286749|7745b74ca805d90a8...|http://data.gdelt...|\n",
      "|  249784|95ea33b0393a85214...|http://data.gdelt...|\n",
      "| 9694304|c0a3d85a1f3a5263b...|http://data.gdelt...|\n",
      "|  250319|47317d0bd9cd56e7b...|http://data.gdelt...|\n",
      "|  304863|37513cccbabc99a9e...|http://data.gdelt...|\n",
      "|10119350|50c3fa1e60385a2ca...|http://data.gdelt...|\n",
      "|  224825|8ca8b3a6b4ff6a960...|http://data.gdelt...|\n",
      "|  298101|68369aec62706a2b5...|http://data.gdelt...|\n",
      "| 9705075|8cd7aa2af725aa02b...|http://data.gdelt...|\n",
      "|  191077|906e6599092855ba2...|http://data.gdelt...|\n",
      "|  252312|de9048e047049b3b6...|http://data.gdelt...|\n",
      "| 9370422|5a674db2a8d110d27...|http://data.gdelt...|\n",
      "|  187888|cee25cd1ab712b7ac...|http://data.gdelt...|\n",
      "|  274338|e12ffacb37d73d9c9...|http://data.gdelt...|\n",
      "|10008236|48764cfe5bc415441...|http://data.gdelt...|\n",
      "|  145719|df1010eb1a3466fef...|http://data.gdelt...|\n",
      "|  255772|781fffb723e8b5595...|http://data.gdelt...|\n",
      "| 8933180|e1fa1f13da14b85e2...|http://data.gdelt...|\n",
      "|  133461|1bcaee75fe0a316da...|http://data.gdelt...|\n",
      "|  246396|2c9bac79d58fc5a14...|http://data.gdelt...|\n",
      "+--------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Long = 288\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampleDF.select(\"url\").repartition(100).foreach( r=> {\n",
    "            val URL = r.getAs[String](0)\n",
    "            val fileName = r.getAs[String](0).split(\"/\").last\n",
    "            val dir = \"./data/\"\n",
    "            val localFileName = dir + fileName\n",
    "            fileDownloader(URL, localFileName)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
